{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":6929936,"sourceType":"datasetVersion","datasetId":3979050}],"dockerImageVersionId":30587,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import nltk\nfrom nltk.tokenize import word_tokenize\nfrom nltk.util import ngrams\nimport re\nfrom tqdm import  tqdm\nimport os\nimport collections\nimport gc\nfrom nltk.probability import ConditionalFreqDist, ConditionalProbDist, MLEProbDist, FreqDist\nimport math","metadata":{"execution":{"iopub.status.busy":"2023-11-22T12:44:05.993493Z","iopub.execute_input":"2023-11-22T12:44:05.993851Z","iopub.status.idle":"2023-11-22T12:44:08.395000Z","shell.execute_reply.started":"2023-11-22T12:44:05.993822Z","shell.execute_reply":"2023-11-22T12:44:08.393100Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"file_path = \"/kaggle/input/amharic-corpus-general/GPAC.txt\"\nn = 3","metadata":{"execution":{"iopub.status.busy":"2023-11-22T12:44:17.007971Z","iopub.execute_input":"2023-11-22T12:44:17.008376Z","iopub.status.idle":"2023-11-22T12:44:17.015148Z","shell.execute_reply.started":"2023-11-22T12:44:17.008345Z","shell.execute_reply":"2023-11-22T12:44:17.013807Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"os.path.exists(file_path)\nif os.path.exists(file_path):\n    print(os.stat(file_path).st_size / (1024 * 1024))\nelse:\n    print(os.path.exists(file_path))","metadata":{"execution":{"iopub.status.busy":"2023-11-22T12:44:18.346308Z","iopub.execute_input":"2023-11-22T12:44:18.346686Z","iopub.status.idle":"2023-11-22T12:44:18.360778Z","shell.execute_reply.started":"2023-11-22T12:44:18.346654Z","shell.execute_reply":"2023-11-22T12:44:18.359536Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"1013.849326133728\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# 1) N-gram language model","metadata":{}},{"cell_type":"markdown","source":"## 1.1 Create n-grams for n=1, 2, 3, 4. You can show sample prints.","metadata":{}},{"cell_type":"code","source":"# Downloading necessary NLTK data\nnltk.download('punkt')\n\ndef process_chunk(chunk, n):\n    tokens = word_tokenize(re.sub(r'\\W+', ' ', chunk))\n    return list(ngrams(tokens, n))\n\ndef read_and_process_in_chunks(file_path, n, chunk_size=100*1024*1024): # read some of the data 100mb\n    with open(file_path, 'r') as file:\n        while True:\n            chunk = file.read(chunk_size)\n            if not chunk:\n                break\n            return process_chunk(chunk, n)\n\n\nall_ngrams = []\nngrams_chunk = read_and_process_in_chunks(file_path, n)\nall_ngrams.extend(ngrams_chunk)\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2023-11-22T12:51:24.845057Z","iopub.execute_input":"2023-11-22T12:51:24.845480Z","iopub.status.idle":"2023-11-22T12:52:54.914060Z","shell.execute_reply.started":"2023-11-22T12:51:24.845447Z","shell.execute_reply":"2023-11-22T12:52:54.912533Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stdout","text":"[nltk_data] Error loading punkt: <urlopen error [Errno -3] Temporary\n[nltk_data]     failure in name resolution>\n","output_type":"stream"},{"execution_count":17,"output_type":"execute_result","data":{"text/plain":"566"},"metadata":{}}]},{"cell_type":"code","source":"print(len(all_ngrams))","metadata":{"execution":{"iopub.status.busy":"2023-11-22T13:00:23.581759Z","iopub.status.idle":"2023-11-22T13:00:23.582136Z","shell.execute_reply.started":"2023-11-22T13:00:23.581970Z","shell.execute_reply":"2023-11-22T13:00:23.581988Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(all_ngrams[:10])","metadata":{"execution":{"iopub.status.busy":"2023-11-22T12:44:48.962757Z","iopub.execute_input":"2023-11-22T12:44:48.963125Z","iopub.status.idle":"2023-11-22T12:44:48.971638Z","shell.execute_reply.started":"2023-11-22T12:44:48.963093Z","shell.execute_reply":"2023-11-22T12:44:48.970359Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"[('ምን', 'መሰላችሁ', 'አንባቢያን'), ('መሰላችሁ', 'አንባቢያን', 'ኢትዮጵያ'), ('አንባቢያን', 'ኢትዮጵያ', 'በተደጋጋሚ'), ('ኢትዮጵያ', 'በተደጋጋሚ', 'ጥሪው'), ('በተደጋጋሚ', 'ጥሪው', 'ደርሷት'), ('ጥሪው', 'ደርሷት', 'ልትታደመው'), ('ደርሷት', 'ልትታደመው', 'ያልቻለችው'), ('ልትታደመው', 'ያልቻለችው', 'የአለም'), ('ያልቻለችው', 'የአለም', 'የእግር'), ('የአለም', 'የእግር', 'ኳስ')]\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## 1.2 Calculate probabilities of n-grams and find the top 10 most likely n-grams for all n.","metadata":{}},{"cell_type":"code","source":"# Create a conditional frequency distribution of Amharic n-grams\ndef create_freq_dist(ngrams):\n    cfreq = ConditionalFreqDist((tuple(ngram[:-1]), ngram[-1]) for ngram in ngrams)\n    return cfreq\n\n# Create a conditional probability distribution using maximum likelihood estimation\ndef create_prob_dist(cfreq):\n    cprob = ConditionalProbDist(cfreq, MLEProbDist)\n    return cprob\n\n\nprobabilites = []\n\ncfreq = create_freq_dist(all_ngrams)\ncprob = create_prob_dist(cfreq)\nprobabilities = cprob\ngc.collect()\n","metadata":{"execution":{"iopub.status.busy":"2023-11-22T12:52:54.917413Z","iopub.execute_input":"2023-11-22T12:52:54.918027Z","iopub.status.idle":"2023-11-22T12:54:44.546991Z","shell.execute_reply.started":"2023-11-22T12:52:54.917980Z","shell.execute_reply":"2023-11-22T12:54:44.545938Z"},"trusted":true},"execution_count":18,"outputs":[{"execution_count":18,"output_type":"execute_result","data":{"text/plain":"0"},"metadata":{}}]},{"cell_type":"code","source":"# Get the top n-grams with the highest probabilities\ndef get_top_ngrams(cfreq, n, top_k):\n    top_ngrams = []\n    for context in cfreq.conditions():\n        if len(context) == n - 1:\n            freq_dist = cfreq[context]\n            top_ngrams.extend([context,x,freq_dist.prob(x)] for x in freq_dist.samples())\n    top_ngrams.sort(key = lambda x:x[2])\n    return top_ngrams[len(top_ngrams)-10:]","metadata":{"execution":{"iopub.status.busy":"2023-11-22T12:54:44.548134Z","iopub.execute_input":"2023-11-22T12:54:44.548451Z","iopub.status.idle":"2023-11-22T12:54:44.555795Z","shell.execute_reply.started":"2023-11-22T12:54:44.548424Z","shell.execute_reply":"2023-11-22T12:54:44.553883Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"top_ngrams = get_top_ngrams(probabilities, 3, 10)\nfor context, word, probability in top_ngrams:\n    print(context,word,probability)","metadata":{"execution":{"iopub.status.busy":"2023-11-22T12:54:44.558196Z","iopub.execute_input":"2023-11-22T12:54:44.558561Z","iopub.status.idle":"2023-11-22T12:55:36.816690Z","shell.execute_reply.started":"2023-11-22T12:54:44.558530Z","shell.execute_reply":"2023-11-22T12:55:36.814758Z"},"trusted":true},"execution_count":20,"outputs":[{"name":"stdout","text":"('ሀብት', 'የሚወድምባት') ግድቦች 1.0\n('የሚወድምባት', 'ግድቦች') ቋሚ 1.0\n('ግድቦች', 'ቋሚ') ዝርያ 1.0\n('መብት', 'የሚገድቡባት') እና 1.0\n('የሚገድቡባት', 'እና') ህዝቦች 1.0\n('ሁኔታ', 'እየወረዱ') ያሉባት 1.0\n('እየወረዱ', 'ያሉባት') አገር 1.0\n('ሆናለች', 'ጆርጅ') ኦርዌል 1.0\n('በ1984', 'መፅፋቸው') ላይ 1.0\n('መፅፋቸው', 'ላይ') እንዳሉት 1.0\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## 1.4 Generate random sentences using n-grams; explain what happens as n increases, based on your output.","metadata":{}},{"cell_type":"code","source":"# Generate a sentence using the n-gram model\ndef generate_sentence(cprob, max_length=20):\n    sentence = []\n    context = cprob.conditions()[0]\n    while len(sentence) < max_length:\n        word = cprob[context].generate()\n        sentence.append(word)\n        context = context[1:] + (word,)\n        if word == '።':\n            break\n    return ' '.join(sentence)","metadata":{"execution":{"iopub.status.busy":"2023-11-22T12:55:36.818358Z","iopub.execute_input":"2023-11-22T12:55:36.818750Z","iopub.status.idle":"2023-11-22T12:55:36.825269Z","shell.execute_reply.started":"2023-11-22T12:55:36.818718Z","shell.execute_reply":"2023-11-22T12:55:36.823766Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"# Generate a sentence using the n-gram model\ngenerated_sentence = generate_sentence(probabilities)\nprint(generated_sentence)","metadata":{"execution":{"iopub.status.busy":"2023-11-22T12:55:36.827262Z","iopub.execute_input":"2023-11-22T12:55:36.827769Z","iopub.status.idle":"2023-11-22T12:55:37.192624Z","shell.execute_reply.started":"2023-11-22T12:55:36.827731Z","shell.execute_reply":"2023-11-22T12:55:37.190675Z"},"trusted":true},"execution_count":22,"outputs":[{"name":"stdout","text":"ወቀሳና ቅጣትን አይለይም ወቅሶ ዝም ነው እንጂ ሰላማዊ ሰልፍ የኢትዮጵያውን አምባገነን ስርአት መጓዝ ይሆናል ወይም የሚሄድበት ሥፍራ ትልቅነት ግርማዊነት ሲያስፈራው\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# 2 Evaluate these Language Models Using Intrinsic Evaluation Method","metadata":{}},{"cell_type":"code","source":"def calculate_perplexity(cpdist, test_set):\n    log_probabilities = [-math.log(cpdist[seq[:-1]].prob(seq[-1],)) for seq in test_set]\n    perplexity = math.exp(sum(log_probabilities) / len(log_probabilities))\n    return perplexity\n\ntest_set = all_ngrams[:30]\n\nperplexity = calculate_perplexity(probabilities, test_set)\nprint(\"Perplexity:\", perplexity)\n","metadata":{"execution":{"iopub.status.busy":"2023-11-22T12:55:37.195028Z","iopub.execute_input":"2023-11-22T12:55:37.195395Z","iopub.status.idle":"2023-11-22T12:55:37.215135Z","shell.execute_reply.started":"2023-11-22T12:55:37.195368Z","shell.execute_reply":"2023-11-22T12:55:37.213830Z"},"trusted":true},"execution_count":23,"outputs":[{"name":"stdout","text":"Perplexity: 2.5852306960357945\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# 3 Evaluate these Language Models Using Extrinsic Evaluation Method","metadata":{}},{"cell_type":"code","source":"def calculate_perplexity(prob_dist, test_data, chunk_size):\n    total_log_probability = 0\n    word_count = 0\n\n    with open(test_data, 'r', encoding='utf-8') as file:\n        chunk = file.read(chunk_size)\n        tokens = nltk.word_tokenize(chunk)\n        word_count += len(tokens)\n\n        for token in tokens:\n            prob = prob_dist[token].prob(token)\n            if prob > 0:\n                total_log_probability += math.log2(prob)\n\n        chunk = file.read(chunk_size)\n\n    avg_log_probability = total_log_probability / word_count\n    perplexity = 2 ** (-avg_log_probability)\n    return perplexity\n\n\nchunk_size = 1024 * 10 * 1024\n\nperplexity = calculate_perplexity(probabilities, file_path, chunk_size)#read another chunk from the file 10mb\nprint(f\"Perplexity: {perplexity:.2f}\")","metadata":{"execution":{"iopub.status.busy":"2023-11-22T13:10:44.203242Z","iopub.execute_input":"2023-11-22T13:10:44.204494Z","iopub.status.idle":"2023-11-22T13:10:54.853732Z","shell.execute_reply.started":"2023-11-22T13:10:44.204444Z","shell.execute_reply":"2023-11-22T13:10:54.852489Z"},"trusted":true},"execution_count":28,"outputs":[{"name":"stdout","text":"Perplexity: 1.00\n","output_type":"stream"}]}]}
