{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":6929936,"sourceType":"datasetVersion","datasetId":3979050}],"dockerImageVersionId":30587,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import nltk\nfrom nltk.tokenize import word_tokenize\nfrom nltk.util import ngrams\nimport re\nfrom tqdm import  tqdm\nimport os\nimport collections\nimport gc\nfrom nltk.probability import ConditionalFreqDist, ConditionalProbDist, MLEProbDist, FreqDist\nimport math","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-11-19T18:26:28.938281Z","iopub.execute_input":"2023-11-19T18:26:28.938871Z","iopub.status.idle":"2023-11-19T18:26:28.944027Z","shell.execute_reply.started":"2023-11-19T18:26:28.938839Z","shell.execute_reply":"2023-11-19T18:26:28.943055Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"file_path = \"/kaggle/input/amharic-corpus-general/GPAC.txt\"\nn = 2","metadata":{"execution":{"iopub.status.busy":"2023-11-19T18:26:31.297376Z","iopub.execute_input":"2023-11-19T18:26:31.297741Z","iopub.status.idle":"2023-11-19T18:26:31.303148Z","shell.execute_reply.started":"2023-11-19T18:26:31.297714Z","shell.execute_reply":"2023-11-19T18:26:31.301959Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"os.path.exists(file_path)\nif os.path.exists(file_path):\n    print(os.stat(file_path).st_size / (1024 * 1024))\nelse:\n    print(os.path.exists(file_path))","metadata":{"execution":{"iopub.status.busy":"2023-11-19T18:26:39.059960Z","iopub.execute_input":"2023-11-19T18:26:39.060501Z","iopub.status.idle":"2023-11-19T18:26:39.069078Z","shell.execute_reply.started":"2023-11-19T18:26:39.060463Z","shell.execute_reply":"2023-11-19T18:26:39.067816Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"1013.849326133728\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# 1) N-gram language model","metadata":{}},{"cell_type":"markdown","source":"## 1.1 Create n-grams for n=1, 2, 3, 4. You can show sample prints.","metadata":{}},{"cell_type":"code","source":"# Downloading necessary NLTK data\nnltk.download('punkt')\n\ndef process_chunk(chunk, n):\n    tokens = word_tokenize(re.sub(r'\\W+', ' ', chunk))\n    return list(ngrams(tokens, n))\n\ndef read_and_process_in_chunks(file_path, n, chunk_size=1024*1024):\n    with open(file_path, 'r') as file:\n        while True:\n            chunk = file.read(chunk_size)\n            if not chunk:\n                break\n            yield process_chunk(chunk, n)\n\n\nall_ngrams = []\nfor ngrams_chunk in read_and_process_in_chunks(file_path, n):\n    all_ngrams.extend(ngrams_chunk)\n    gc.collect()\n","metadata":{"execution":{"iopub.status.busy":"2023-11-19T18:26:47.289769Z","iopub.execute_input":"2023-11-19T18:26:47.290323Z","iopub.status.idle":"2023-11-19T18:43:56.791105Z","shell.execute_reply.started":"2023-11-19T18:26:47.290281Z","shell.execute_reply":"2023-11-19T18:43:56.789807Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"[nltk_data] Error loading punkt: <urlopen error [Errno -3] Temporary\n[nltk_data]     failure in name resolution>\n","output_type":"stream"}]},{"cell_type":"code","source":"print(len(all_ngrams))","metadata":{"execution":{"iopub.status.busy":"2023-11-19T18:43:56.793689Z","iopub.execute_input":"2023-11-19T18:43:56.794177Z","iopub.status.idle":"2023-11-19T18:43:56.801426Z","shell.execute_reply.started":"2023-11-19T18:43:56.794139Z","shell.execute_reply":"2023-11-19T18:43:56.799853Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"77759851\n","output_type":"stream"}]},{"cell_type":"code","source":"print(all_ngrams[:10])","metadata":{"execution":{"iopub.status.busy":"2023-11-19T18:43:56.803119Z","iopub.execute_input":"2023-11-19T18:43:56.804262Z","iopub.status.idle":"2023-11-19T18:43:56.813976Z","shell.execute_reply.started":"2023-11-19T18:43:56.804197Z","shell.execute_reply":"2023-11-19T18:43:56.812985Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"[('ምን', 'መሰላችሁ'), ('መሰላችሁ', 'አንባቢያን'), ('አንባቢያን', 'ኢትዮጵያ'), ('ኢትዮጵያ', 'በተደጋጋሚ'), ('በተደጋጋሚ', 'ጥሪው'), ('ጥሪው', 'ደርሷት'), ('ደርሷት', 'ልትታደመው'), ('ልትታደመው', 'ያልቻለችው'), ('ያልቻለችው', 'የአለም'), ('የአለም', 'የእግር')]\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## 1.2 Calculate probabilities of n-grams and find the top 10 most likely n-grams for all n.","metadata":{}},{"cell_type":"code","source":"# Create a conditional frequency distribution of Amharic n-grams\ndef create_freq_dist(ngrams):\n    cfreq = ConditionalFreqDist((tuple(ngram[:-1]), ngram[-1]) for ngram in ngrams)\n    return cfreq\n\n# Create a conditional probability distribution using maximum likelihood estimation\ndef create_prob_dist(cfreq):\n    cprob = ConditionalProbDist(cfreq, MLEProbDist)\n    return cprob\n\n\nprobabilites = []\n\ncfreq = create_freq_dist(all_ngrams)\ncprob = create_prob_dist(cfreq)\nprobabilities = cprob\ngc.collect()\n\n    ","metadata":{"execution":{"iopub.status.busy":"2023-11-19T18:49:13.614053Z","iopub.execute_input":"2023-11-19T18:49:13.614471Z","iopub.status.idle":"2023-11-19T18:54:12.114075Z","shell.execute_reply.started":"2023-11-19T18:49:13.614437Z","shell.execute_reply":"2023-11-19T18:54:12.112784Z"},"trusted":true},"execution_count":15,"outputs":[{"execution_count":15,"output_type":"execute_result","data":{"text/plain":"0"},"metadata":{}}]},{"cell_type":"code","source":"def get_top_ngrams(cpd, n):\n    top_ngrams = []\n    for context in cpd.conditions():\n        for word in cpd[context].samples():\n            probability = cpd[context].prob(word)\n            top_ngrams.append((context + (word,), probability))\n    \n    top_ngrams.sort(key=lambda x: x[1], reverse=True)\n    top_ngrams = top_ngrams[:10]\n    return top_ngrams","metadata":{"execution":{"iopub.status.busy":"2023-11-19T18:48:39.439863Z","iopub.status.idle":"2023-11-19T18:48:39.440361Z","shell.execute_reply.started":"2023-11-19T18:48:39.440144Z","shell.execute_reply":"2023-11-19T18:48:39.440164Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"top_ngrams = get_top_ngrams(probabilities, 10)\nprint(top_ngrams)","metadata":{"execution":{"iopub.status.busy":"2023-11-19T18:48:39.442054Z","iopub.status.idle":"2023-11-19T18:48:39.442583Z","shell.execute_reply.started":"2023-11-19T18:48:39.442348Z","shell.execute_reply":"2023-11-19T18:48:39.442371Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 1.3 What is the probability of the sentence. \"ኢትዮጵያ ታሪካዊ ሀገር ናት \". You can also try more sentences.","metadata":{}},{"cell_type":"code","source":"sequence = (\"ኢትዮጵያ\" ,\"ታሪካዊ\" ,\"ሀገር\", \"ናት\")\nprobability = 1.0\nbigram_counts = cfreq\n\n\n# Calculate the probability of the sequence\nfor i in range(1, len(sequence)):\n    prev_word = sequence[i-1]\n    current_word = sequence[i]\n    count = bigram_counts[prev_word][current_word]\n    total_count = bigram_counts[prev_word].N()\n    conditional_probdist = MLEProbDist(bigram_counts[prev_word])\n    conditional_prob = conditional_probdist.prob(current_word)\n    probability *= conditional_prob\n\n    \nprint(\"Probability:\", probability)","metadata":{"execution":{"iopub.status.busy":"2023-11-19T18:48:39.444056Z","iopub.status.idle":"2023-11-19T18:48:39.444536Z","shell.execute_reply.started":"2023-11-19T18:48:39.444305Z","shell.execute_reply":"2023-11-19T18:48:39.444325Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 1.4 Generate random sentences using n-grams; explain what happens as n increases, based on your output.","metadata":{}},{"cell_type":"code","source":"# Generate a sentence using the n-gram model\ndef generate_sentence(cprob, max_length=20):\n    sentence = []\n    context = cprob.conditions()[0]\n    while len(sentence) < max_length:\n        word = cprob[context].generate()\n        sentence.append(word)\n        context = context[1:] + (word,)\n        if word == '።':\n            break\n    return ' '.join(sentence)","metadata":{"execution":{"iopub.status.busy":"2023-11-19T18:48:39.445381Z","iopub.status.idle":"2023-11-19T18:48:39.445750Z","shell.execute_reply.started":"2023-11-19T18:48:39.445575Z","shell.execute_reply":"2023-11-19T18:48:39.445592Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Generate a sentence using the n-gram model\ngenerated_sentence = generate_sentence(probabilities)\nprint(generated_sentence)","metadata":{"execution":{"iopub.status.busy":"2023-11-19T18:48:39.448275Z","iopub.status.idle":"2023-11-19T18:48:39.448687Z","shell.execute_reply.started":"2023-11-19T18:48:39.448447Z","shell.execute_reply":"2023-11-19T18:48:39.448494Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 2 Evaluate these Language Models Using Intrinsic Evaluation Method","metadata":{}},{"cell_type":"code","source":"def calculate_perplexity(cpdist, test_set):\n    log_probabilities = [-math.log(cpdist[seq[:-1]].prob(seq[-1],)) for seq in test_set]\n    perplexity = math.exp(sum(log_probabilities) / len(log_probabilities))\n    return perplexity\n\ntest_set = all_ngrams[:30]\n\nperplexity = calculate_perplexity(probabilities, test_set)\nprint(\"Perplexity:\", perplexity)\n","metadata":{"execution":{"iopub.status.busy":"2023-11-19T18:54:12.116540Z","iopub.execute_input":"2023-11-19T18:54:12.117054Z","iopub.status.idle":"2023-11-19T18:54:12.129533Z","shell.execute_reply.started":"2023-11-19T18:54:12.117014Z","shell.execute_reply":"2023-11-19T18:54:12.128066Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stdout","text":"Perplexity: 191.34567451384368\n","output_type":"stream"}]}]}
